x output: 
  html_document: 
<<<<<<< HEAD
    highlight: tango
    theme: readable
=======
    fig_width: 9
    fig_height: 7
>>>>>>> ffdce05e27cdec45aa069f55b77b5c4609b13675
---
## YAML
---
author: "Chinny90"
<<<<<<< HEAD
title: "Applied_Health_Practicals0"
=======
title: "Applied_Health_Practicals"
>>>>>>> ffdce05e27cdec45aa069f55b77b5c4609b13675
output: html_document
date: "2024-06-04"
---
---
author:"ChinnyB00928009"
title: "Practical1"
output: html_document
date: "2024-06-04"

---
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(tidytext)
library(textstem)
library(clinspacy)
library(topicmodels)
library('reshape2')
library(stringr)

```

This practical is based on exploratory data analysis, named entity recognition, and topic modelling of unstructured medical note free-text data derived from electronic medical records (EMR).  Real EMR data is very difficult to access without a specific need/request so this data set is derived from [medical transcription](https://mtsamples.com/) data instead.  I'll also caveat that the options of natural language processing (NLP) in R are far inferior to those available in Python. 

First, install the packages in the setup block (`install.packages(c("readr", "dplyr", "tidyr", "ggplot2", "tidtext", "textstem", "clinspacy", "topicmodels", "reshape2"))`).

Note: To try and make it clearer which library certain functions are coming from clearer, I'll try to do explicit imports throughout this notebook.

## Data Parsing

After that we can grab the dataset directly from the `clinspacy` library.

```{r}
raw.data <- clinspacy::dataset_mtsamples()
dplyr::glimpse(raw.data)
```

There is no explanation or data dictionary with this dataset, which is a surprisingly common and frustrating turn of events!  

**1** Using the output of dplyr's `glimpse` command (or rstudio's data viewer by clicking on `raw.data` in the Environment pane) provide a description of what you think each variable in this dataset contains.

note_id:integer variable that serves as a unique identifier for each note entry. For example; 1

description: this character variable holds a brief description of the patients' presentation and reason for visit. For example; A 23-year-old white female presents with complaint of allergies.

medical_specialty: this caharacter variable contains information that specifies medical field relevant to the note.For example;Allergy / Immunology

sample_name:holds the specific medical procedure as detailed in the note. For example; Allergic Rhinitis

transcription: This is an unstructured detailed note about the patient, diagnosis, results of examination and treatment plan. SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.

keywords: This holds keywords or tokens from the note. allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,

Let's see how many different medical specialties are featured in these notes: 
```{r}

raw.data %>% dplyr::select(medical_specialty) %>% dplyr::n_distinct()
```
There are 40 different medical specialties featured in the notes within the dataset.

So, how many transcripts are there from each specialty:


```{r}
ggplot2::ggplot(raw.data, ggplot2::aes(y=medical_specialty)) + ggplot2::geom_bar() + labs(x="Document Count", y="Medical Speciality")
```



Each bar represents a medical specialty, and the height of the bar indicates the number of transcripts associated with that specialty.


Let's make our life easier and filter down to 3 specialties: a diagonstic/lab, a medical, and a surgical specialty

```{r} 
filtered.data <- raw.data %>% dplyr::filter(medical_specialty %in% c("Orthopedic", "Radiology", "Surgery")) 
#print(filtered.data)
```

## Text Processing

Let's now apply our standard pre-processing to the transcripts from these specialties.  
We are going to use the `tidytext` package to tokenise the transcript free-text.  
Let's remove stop words first. e.g., "the", "of", "to", and so forth. These are known as stop words and we can remove them relative easily using a list from  `tidytext::stop_words` and `dplyr::anti_join()`

```{r}

analysis.data <- filtered.data %>%
  unnest_tokens(word, transcription) %>%
  mutate(word = str_replace_all(word, "[^[:alnum:]]", "")) %>%
  filter(!str_detect(word, "[0-9]")) %>%
  anti_join(stop_words) %>%
  group_by(note_id) %>%
  summarise(transcription = paste(word, collapse = " ")) %>%
  left_join(select(filtered.data, -transcription), by = "note_id")
print(analysis.data)
```


Now let's tokenize the `transcription` to words (unigram) 
By default this tokenises to words but other options include characters, n-grams, sentences, lines, paragraphs, or separation around a regular expression.

```{r}
tokenized.data.unigram <- analysis.data %>% tidytext::unnest_tokens(word, transcription, to_lower=TRUE)
print (tokenized.data.unigram)
```

You can also do bi-grams
```{r}
tokenized.data <- analysis.data %>% tidytext::unnest_tokens(ngram, transcription, token = "ngrams", n=2, to_lower = TRUE)
print (tokenized.data)
```

How many stop words are there in `tidytext::stop_words` from each lexicon?
```{r}
tidytext::stop_words %>% dplyr::group_by(lexicon) %>% dplyr::distinct(word) %>% dplyr::summarise(n=dplyr::n())
```

**2** How many unique unigrams are there in the transcripts from each specialty:

```{r}

library(dplyr)
library(tidytext)
library(stringr)

filtered.data <- raw.data %>%
  dplyr::filter(medical_specialty %in% c("Orthopedic", "Radiology", "Surgery"))

tokenized.data.unigram <- filtered.data %>%
  tidytext::unnest_tokens(word, transcription, to_lower = TRUE) %>%
  mutate(word = str_replace_all(word, "[^[:alnum:]]", "")) %>%
  filter(!str_detect(word, "[0-9]")) %>%
  anti_join(tidytext::stop_words, by = "word")

unique_unigrams_by_specialty <- tokenized.data.unigram %>%
  group_by(medical_specialty) %>%
  summarise(unique_unigrams = n_distinct(word))


print(unique_unigrams_by_specialty)

```

Let's plot some distribution of unigram tokens (words)


```{r}
word_counts <- tokenized.data.unigram %>%
    group_by(word) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    arrange(desc(count))

count_distribution <- word_counts %>%
  group_by(count) %>%
  summarise(num_words = n()) %>%
  ungroup()
 
 ggplot2::ggplot(count_distribution, aes(x = count, y = num_words)) +
  geom_point() +
  labs(title = "Scatter Plot of Count Distribution",
       x = "Count of Unique Words",
       y = "Number of Words")
```


Each point on the x-axis represents a specific count of unique words. At point  x = 5000, it means there are 5000 unique words that appear a certain number of times in the dataset.The y-axis shows the number of words (unigram tokens) that have the count specified on the x-axis. Each point on the y-axis represents the number of words that have the corresponding count of unique words as shown on the x-axis. At point y = 500, it means there are 500 words that have a count equal to the value shown on the x-axis.Each point on the plot represents a combination of a count of unique words (x-coordinate) and the number of words with that count (y-coordinate). 

Let's plot some distribution of bigram tokens (words)


```{r}
word_counts <- tokenized.data %>%
    group_by(ngram) %>%
    summarise(count = n()) %>%
    ungroup() %>%
    arrange(desc(count))

count_distribution <- word_counts %>%
  group_by(count) %>%
  summarise(num_words = n()) %>%
  ungroup()
 
 ggplot2::ggplot(count_distribution, aes(x = count, y = num_words)) +
  geom_point() +
  labs(title = "Scatter Plot of Count Distribution",
       x = "Count of Unique Bigrams",
       y = "Number of Words")
```


Each point on the x-axis represents a specific count of unique words. At point  x = 250, it means there are 250 unique words that appear a certain number of times in the dataset.The y-axis shows the number of words (unigram tokens) that have the count specified on the x-axis. Each point on the y-axis represents the number of words that have the corresponding count of unique words as shown on the x-axis. At point y = 6500, it means there are 6500 words that have a count equal to the value shown on the x-axis.Each point on the plot represents a combination of a count of unique words (x-coordinate) and the number of words with that count (y-coordinate). 

**3** How many unique bi-grams are there in each category without stop words and numbers?
```{r}

library(dplyr)
library(tidytext)
library(stringr)
# Tokenize the 'transcription' column into bi-grams
tokenized_bigrams <- analysis.data %>%
  tidytext::unnest_tokens(bigram, transcription, token = "ngrams", n = 2, to_lower = TRUE) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
  filter(!str_detect(word1, "\\d"), !str_detect(word2, "\\d")) %>%
  unite(bigram, word1, word2, sep = " ")

# Count unique bi-grams for each medical specialty
unique_bigrams <- tokenized_bigrams %>%
  group_by(medical_specialty, bigram) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(medical_specialty) %>%
  summarise(unique_bigrams = n_distinct(bigram)) %>%
  arrange(desc(unique_bigrams))

# Print results
print("Unique Bi-grams by Medical Specialty")
print(unique_bigrams)
```



Sometimes we are interested in tokenising/segmenting things other than words like whole sentences or paragraphs.  

**4** How many unique sentences are there in each category? Hint: use `?tidytext::unnest_tokens` to see the documentation for this function.
```{r}
library(dplyr)
library(tidytext)
library(stringr)

# Tokenize the 'transcription' column into sentences
tokenized_sentences <- analysis.data %>%
  tidytext::unnest_tokens(sentence, transcription, token = "sentences", to_lower = TRUE)

# Count unique sentences for each medical specialty
unique_sentences <- tokenized_sentences %>%
  group_by(medical_specialty, sentence) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(medical_specialty) %>%
  summarise(unique_sentences = n_distinct(sentence)) %>%
  arrange(desc(unique_sentences))

# Print results
print("Unique Sentences by Medical Specialty")
print(unique_sentences)
```

Now that we've tokenized to words and removed stop words, we can find the most commonly word used within each category:

```{r}
tokenized.data %>%
  dplyr::group_by(medical_specialty) %>%
  dplyr::count(ngram, sort = TRUE) %>%
  dplyr::top_n(5)
```


The most commonly word use in surgery and orthopedic is "prepped draped". In radiology, it is "carotid artery".

We should lemmatize the tokenized words to prevent over counting of similar words before further analyses.  
Annoyingly, `tidytext` doesn't have a built-in lemmatizer.

**5** Do you think a general purpose lemmatizer will work well for medical data? Why might it not?

I do not think a general purpose lemmatizer would work well for a medical data set because medical dataset most times contains specialized vocabulary, compound words, and context-sensitive terms used in medical documentation. General lemmatizer would struggle to make ot the meaning of medical terms even given the context and might not accurately lemmatize words in a way that reflects their medical meanings. 

Unfortunately, a specialised lemmatizer like in `clinspacy` is going to be very painful to install so we will just use a simple lemmatizer for now:

```{r}
lemmatized.data <- tokenized.data %>% dplyr::mutate(lemma=textstem::lemmatize_words(ngram))
```

We can now calculate the frequency of lemmas within each specialty and note.
```{r}
lemma.freq <- lemmatized.data %>% 
  dplyr::count(medical_specialty, lemma) %>%
  dplyr::group_by(medical_specialty) %>% 
  dplyr::mutate(proportion = n / sum(n)) %>%
  tidyr::pivot_wider(names_from = medical_specialty, values_from = proportion) %>%
  tidyr::pivot_longer(`Surgery`:`Radiology`,
               names_to = "medical_specialty", values_to = "proportion")
```

And plot the relative proportions 
```{r}

ggplot2::ggplot(lemma.freq, ggplot2::aes(x=proportion, 
                                         y=`Orthopedic`,
                                         color=abs(`Orthopedic` - proportion))) + 
  ggplot2::geom_abline(color="gray40", lty=2) +
  ggplot2::geom_jitter(alpha=0.1, size=2.5, width=0.3, height=0.3) +
  ggplot2::geom_text(ggplot2::aes(label=lemma), check_overlap=TRUE, vjust=1.5) +
  ggplot2::scale_x_log10(labels=scales::percent_format()) + 
  ggplot2::scale_y_log10(labels=scales::percent_format()) + 
  ggplot2::scale_color_gradient(limits=c(0, 0.001), low="darkslategray4", high="gray75") +
  ggplot2::facet_wrap(~medical_specialty, ncol = 2) +
  ggplot2::theme(legend.position="none") +
  ggplot2:: labs(y="Orthopedic", x = NULL)
```
**6** What does this plot tell you about the relative similarity of lemma frequencies between Surgery and Orthopedic and between radiology and Surgery? Based on what these specialties involve, is this what you would expect?

The plot suggests that lemma frequencies are highly similar between Surgery and Orthopedic, due to overlap in terminology for both specialty. For Surgery and Radiology, though not as close as Surgery and Orthopedic, there is still a moderate level of similarity in lemma frequencies, indicating shared aspects of patient care and clinical practice. These findings is what I expected given the nature of the specialties involved and the domains of knowledge they encompass.
The X-Axis represents the proportion of each lemma within each medical specialty while Y-Axis represents the proportion of each lemma specifically within the "Orthopedic" specialty, also on a logarithmic scale.
The gray dashed line represents the line of equality (y = x). Points lying on this line indicate lemmas that have the same proportion in both the compared specialty and the "Orthopedic" specialty.
Each point represents a lemma.The position of a point indicates how the lemma's proportion in a particular specialty compares to its proportion in "Orthopedic". If a point is above the line, the lemma is more frequent in "Orthopedic" than in the other specialty. If it is below the line, the lemma is less frequent in "Orthopedic".
Points that are far from the diagonal line (either above or below) highlight lemmas that have significant differences in their proportions between "Orthopedic" and the other specialties. These lemmas might be particularly characteristic of one specialty over another.

**7** Modify the above plotting code to do a direct comparison of Surgery and Radiology (i.e., have Surgery or Radiology on the Y-axis and the other 2 specialties as the X facets)


```{r}
library(dplyr)
library(tidytext)
library(textstem)
library(ggplot2)
library(tidyr)

tokenized.data <- analysis.data %>% 
  tidytext::unnest_tokens(ngram, transcription, token = "ngrams", n = 2, to_lower = TRUE) %>% 
  dplyr::mutate(lemma = textstem::lemmatize_words(ngram))

lemma.freq <- tokenized.data %>% 
  dplyr::count(medical_specialty, lemma) %>%
  dplyr::group_by(medical_specialty) %>% 
  dplyr::mutate(proportion = n / sum(n)) %>%
  tidyr::pivot_wider(names_from = medical_specialty, values_from = proportion)


lemma.freq.filtered <- lemma.freq %>%
  dplyr::filter(!is.na(Surgery) & !is.na(Radiology)) %>%
  dplyr::select(lemma, Surgery, Radiology)

# Plot the relative proportions for Surgery and Radiology comparison
ggplot2::ggplot(lemma.freq.filtered, ggplot2::aes(x = Radiology, 
                                                  y = Surgery, 
                                                  color = abs(Surgery - Radiology))) + 
  ggplot2::geom_abline(color = "gray40", lty = 2) +
  ggplot2::geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  ggplot2::geom_text(ggplot2::aes(label = lemma), check_overlap = TRUE, vjust = 1.5) +
  ggplot2::scale_x_log10(labels = scales::percent_format()) + 
  ggplot2::scale_y_log10(labels = scales::percent_format()) + 
  ggplot2::scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  ggplot2::theme(legend.position = "none") +
  ggplot2::labs(y = "Surgery", x = "Radiology")
```


This plot shows that for most lemmas, the proportion in Surgery is higher than in Radiology. For lemmas closer to the diagonal line, indicates similar frequencies in both specialties, while those below indicate higher frequencies in Surgery.The color gradient represents the absolute difference in proportions between the two specialties. Lighter shades indicate smaller differences in lemma proportions. Thus, most lemmas do not differ greatly in their relative frequencies between Surgery and Radiology, but they are consistently more common in Surgery.

### TF-IDF Normalisation

Maybe looking at lemmas across all notes in a specialty is misleading, what if we look at lemma frequencies across a specialty.

```{r}
lemma.counts <- lemmatized.data %>% dplyr::count(medical_specialty, lemma)
total.counts <- lemma.counts %>% 
                      dplyr::group_by(medical_specialty) %>% 
                      dplyr::summarise(total=sum(n))

all.counts <- dplyr::left_join(lemma.counts, total.counts)
#all.counts <- left_join(lemma.counts, total.counts, by = "medical_specialty")
```
Now we can calculate the term frequency / invariant document frequency (tf-idf):

```{r}
all.counts.tfidf <- tidytext::bind_tf_idf(all.counts, lemma, medical_specialty, n) 
```

We can then look at the top 10 lemma by tf-idf within each specialty:

```{r}
all.counts.tfidf %>% dplyr::group_by(medical_specialty) %>% dplyr::slice_max(order_by=tf_idf, n=10)
```

**8** Are there any lemmas that stand out in these lists? Why or why not? 

The lemma with the highest tf-idf scores in each the 3 identified specialty are;lemma "myocardial perfusion" in Radiology with the highest tf_idf of 0.0008513375. Lemma "range motion" is the highest in Orthopedic with tf-idf score of 0.0005764278. In Surgery specialty,"anterior chamber"is the most important lemma with tf-idf score of 0.0004271531. TF-IDF scores indicate the importance of each lemma within a specific medical specialty compared to all other medical specialties.  Interestingly, the lemma "myocardial perfusion" with the highest tf-idf across all three specialty happens to fall in the Radiology specialty. Radiology due to referring to imaging for many different diagnoses/reasons and "myocardial perfusion" referring to the process of assessing blood flow to the heart muscle, typically through imaging techniques.


We can look at transcriptions in full using these lemmas to check how they are used with `stringr::str_detect`

```{r}
analysis.data %>% dplyr::select(medical_specialty, transcription) %>% dplyr::filter(stringr::str_detect(transcription, 'steri strips')) %>% dplyr::slice(1)
```

**9** Extract an example of one of the other "top lemmas" by modifying the above code

```{r}
analysis.data %>% dplyr::select(medical_specialty, transcription) %>% dplyr::filter(stringr::str_detect(transcription, 'myocardial perfusion')) %>% dplyr::slice(1)
```


## Topic Modelling

In NLP, we often have collections of documents (in our case EMR transcriptions) that we’d like to divide into groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data.

Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.


- Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”


- Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.

LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document. There are a number of existing implementations of this algorithm, and we’ll explore one of them in depth.

First lets calculate a term frequency matrix for each transcription:
```{r}
lemma.counts <- lemmatized.data %>% dplyr::count(note_id, lemma)
total.counts <- lemma.counts %>% 
                      dplyr::group_by(note_id) %>% 
                      dplyr::summarise(total=sum(n))

all.counts <- dplyr::left_join(lemma.counts, total.counts)

emr.dcm <- all.counts %>% tidytext::cast_dtm(note_id, lemma, n)
```


Then we can use LDA function to fit a 5 topic (`k=5`) LDA-model
```{r}
emr.lda <- topicmodels::LDA(emr.dcm, k=5, control=list(seed=42))
emr.topics <- tidytext::tidy(emr.lda, matrix='beta')
```

Then we can extract the top terms per assigned topic:
```{r}
top.terms <- emr.topics %>% dplyr::group_by(topic) %>% 
  dplyr::slice_max(beta, n=10) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(topic, -beta)


top.terms %>% 
  dplyr::mutate(term=tidytext::reorder_within(term, beta, topic)) %>% 
  ggplot2::ggplot(ggplot2::aes(beta, term, fill=factor(topic))) + 
    ggplot2::geom_col(show.legend=FALSE) + 
    ggplot2::facet_wrap(~ topic, scales='free')  +
    ggplot2::theme(axis.text.x = element_text(angle = 45,vjust = 1,hjust = 1)) +
    tidytext::scale_y_reordered()
```


Now we can ask how well do these assigned topics match up to the medical specialties from which each of these transcripts was derived.

```{r}
specialty_gamma <- tidytext::tidy(emr.lda, matrix='gamma')

# we need to join in the specialty from the note_id
note_id_specialty_mapping <- lemmatized.data %>%
  dplyr::mutate(document=as.character(note_id)) %>% 
  dplyr::select(document, medical_specialty) %>% 
  dplyr::distinct()

specialty_gamma <- dplyr::left_join(specialty_gamma, note_id_specialty_mapping)
```


```{r}
specialty_gamma %>%
  dplyr::mutate(medical_specialty = reorder(medical_specialty, gamma * topic)) %>%
  ggplot2::ggplot(ggplot2::aes(factor(topic), gamma)) +
  ggplot2::geom_boxplot() +
  ggplot2::facet_wrap(~ medical_specialty) +
  ggplot2::labs(x = "topic", y = expression(gamma))
```


Interestingly, Surgery, Orthopedic, and Radiology assign mostly to a single topics. We'd possibly expect this from radiology due to referring to imaging for many different diagnoses/reasons. 
However, this may all just reflect we are using too few topics in our LDA to capture the range of possible assignments. 

**10** Repeat this with a 6 topic LDA, do the top terms from the 5 topic LDA still turn up? How do the specialties get split into sub-topics?

First lets calculate a term frequency matrix for each transcription:
```{r}

lemma.counts <- lemmatized.data %>% dplyr::count(note_id, lemma)
total.counts <- lemma.counts %>% 
                      dplyr::group_by(note_id) %>% 
                      dplyr::summarise(total=sum(n))

all.counts <- dplyr::left_join(lemma.counts, total.counts)

emr.dcm <- all.counts %>% tidytext::cast_dtm(note_id, lemma, n)
```

```{r}
#Then fit a 6-topic LDA model
emr.lda_6 <- topicmodels::LDA(emr.dcm, k = 6, control = list(seed = 42))
emr.topics_6 <- tidytext::tidy(emr.lda_6, matrix = 'beta')

# Then extract top terms per assigned topic
top.terms_6 <- emr.topics_6 %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualize top terms
top.terms_6 %>% 
  mutate(term = tidytext::reorder_within(term, beta, topic)) %>% 
  ggplot2::ggplot(ggplot2::aes(beta, term, fill = factor(topic))) + 
  ggplot2::geom_col(show.legend = FALSE) + 
  ggplot2::facet_wrap(~ topic, scales = 'free')  +
  ggplot2::theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  tidytext::scale_y_reordered()
```

```{r}


# Then, we match topics to medical specialties
specialty_gamma_6 <- tidytext::tidy(emr.lda_6, matrix = 'gamma')

# We join in the specialty from the note_id
note_id_specialty_mapping <- lemmatized.data %>%
  mutate(document = as.character(note_id)) %>% 
  select(document, medical_specialty) %>% 
  distinct()

specialty_gamma_6 <- left_join(specialty_gamma_6, note_id_specialty_mapping)

# Then visualize specialties split into sub-topics
specialty_gamma_6 %>%
  mutate(medical_specialty = reorder(medical_specialty, gamma * topic)) %>%
  ggplot2::ggplot(ggplot2::aes(factor(topic), gamma)) +
  ggplot2::geom_boxplot() +
  ggplot2::facet_wrap(~ medical_specialty) +
  ggplot2::labs(x = "topic", y = expression(gamma))
```


As expected, only Radiology assign mostly to a single topics. This could be due to referring to imaging for many different diagnoses/reasons.  Compared to the earlier, this could indicate that the minimum required number  of topics in our LDA was used to capture the range of possible assignments. Only prepped drapped and coronary artery made an appearance as the top terms with prepped drapped appearing 5 of the 6.

## Credits

Examples draw heavily on material (and directly quotes/copies text) from Julia Slige's `tidytext` textbook.

